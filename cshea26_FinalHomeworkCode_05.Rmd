---
title: "Boots For Days!"
author: "Carly S McDermott"
date: "2025-04-01"
output:
  rmarkdown::html_document:
    theme: readable
    toc: yes
    toc_depth: 2
    toc_float: 
      collapsed: yes
      smooth_scroll: yes
---
<!-- adding image of boots hehe -->           
<p align="center"> 
  <img src="https://media3.giphy.com/media/v1.Y2lkPTc5MGI3NjExbnoyd3JweWd4bDJqZGk3dzB5Mml6ZHZqZ3U3dWIwNTl5Y3RobjRrMSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/26FmQPtkD2oatMC9G/giphy.gif" alt="GIF">
</p>

<style>
  body {
    background-color: #f3f3f3;
    color: #333333; /* i changed the text color to dark gray for contrast */
  }
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Part 1
Using the ‚ÄúKamilarAndCooperData.csv‚Äù dataset, run a linear regression looking at log(HomeRange_km2) in relation to log(Body_mass_female_mean) and report your ùõΩ coefficients (slope and intercept).
```{r}
#start by loading my packages and data
library(curl) #curl is used to transfer my data

f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN588_Fall21/KamilarAndCooperData.csv")
data <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = FALSE)
head(data) #overall data looks good 
```
```{r}
# running linear regression - log transformed it myself in the reg1 function:
reg1 <-lm(data=data, log(data$HomeRange_km2) ~ log(data$Body_mass_female_mean))
summary(reg1)
```
- We learn from the summary of reg1 above that the model produces an beta intercept of -9.44123 and the slope is 1.03643.

## Part 2
Then, use bootstrapping to sample from your data 1000 times with replacement, each time fitting the same model and calculating the same coefficients. This generates a sampling distribution for each ùõΩ coefficient.

```{r}
library(boot) # bootstrapping package

# creating a function to return slope and beta value coefficients
boots <- function(formula, data, indices) {
  datar <- data[indices, ]
  fit <- lm(formula, data = datar)  
  return(coef(fit))  
}
set.seed(1234)

# this represents 1000 replications
boots1 <- boot(data = data, statistic = boots, R = 1000, formula = log(HomeRange_km2) ~ log(Body_mass_female_mean))
print(boots1)

# shows the distribution of the coefficients (intercept & slope)
plot(boots1, index = 1)  #intercept
plot(boots1, index = 2)  #slope
```

- Estimate the standard error for each of your ùõΩ coefficients as the standard deviation of the sampling distribution from your bootstrap and determine the 95% CI for each of your ùõΩ coefficients based on the appropriate quantiles from your sampling distribution.
```{r}
# (according to R In Action) bca is preferable here because it provides an interval that makes simple adjustments for bias
boot.ci(boots1, type = "bca", index = 1)  # 95% CI for intercept
boot.ci(boots1, type = "bca", index = 2)  # 95% CI for slope
```
^ i really like this function/command and makes visualizing these confidence intervals very clear and straightforward!

- How does the former compare to the SE estimated from your entire dataset using the formula for standard error implemented in lm()?

**Differences In Analysis of SE:** 

- In the original: standard error is estimated from the original model (reg1) which is just is based on the assumptions of linear regression (including normality + homoscedasticity)
- In bootstrapped model: standard error is standard deviation of the bootstrapped coefficient estimates across the 1000 datasets we sampled 1000
```{r}
summary(reg1)$coefficients[,2]  # se from lm() model (part 1) (,2 because this only pulls out se-which is column 2)

# standard errors from bootstrapping (part 2)
sd(boots1$t[,1])  # se of intercept
sd(boots1$t[,2])  # se of slope
```
We can see here that the se for the beta intercept and slope would be similar (original model is slightly higher)

REG1 (estimate):

- ùõΩ coefficient: 0.67293459 
- slope: 0.08487709 

BOOTS:

- ùõΩ coefficient: 0.5728366
- slope: 0.07308898

**Comparison**: Basically these values are very similar. This makes sense as if the original dataset follows the assumptions of linear regression, the standard error from both methods should be relatively similar. Say our data was not normal (i.e. not normal residual) the bootstrap standard error might deviate from the estimate of the first model.


- How does the latter compare to the 95% CI estimated from your entire dataset?
```{r}
#creating confidence interval for entire model
confint(reg1) #level is already set to .95 
```
REG1 (estimate):

- 95% CI Interval for ùõΩ: **(-10.7720889, -8.110374)**
- 95% CI Interval for slope: **(0.8685707  1.204292)**

BOOTS:

- 95% CI Interval for ùõΩ: **(-10.680,  -8.365)**
- 95% CI Interval for slope: **(0.904,  1.195)** 

**Comparison**: The 95% CI's from both methods are pretty close, supporting that the data likely meet the assumptions of linear regression. we see slope interval of 95% CIs for the bootstrapping is slightly narrower. 

## EXTRA CREDIT

*Write a FUNCTION that takes as its arguments a dataframe, ‚Äúd‚Äù, a linear model, ‚Äúm‚Äù (as a character string, e.g., ‚ÄúlogHR~logBM‚Äù), a user-defined confidence interval level, ‚Äúconf.level‚Äù (with default = 0.95), and a number of bootstrap replicates, ‚Äún‚Äù (with default = 1000). Your function should return a dataframe that includes: beta coefficient names; beta coefficients, standard errors, and upper and lower CI limits for the linear model based on your entire dataset; and mean beta coefficient estimates, SEs, and CI limits for those coefficients based on your bootstrap.*

```{r}
#ignore here: making variables that follow format in question (logHR~logBM)
logHR <- log(data$HomeRange_km2)
logBM <- log(data$Body_mass_female_mean)

# creating my function - dataframe + linear model + CI + conf.level + replicates: 
bootstrap_function <- function(d, m, conf.level = 0.95, n = 1000) {
  # converts linear model to formula object
  model_formula <- as.formula(m)
 
  # fitting it to my original regression module
  og_model <- lm(model_formula, data = d)
  og_summary <- summary(og_model)
  og_ci <- confint(og_model, level = conf.level) # calculating CIs
  
  #making bootstrap replicates for coefficients
  boot_coefs <- replicate(n, {
    sampled_data <- d[sample(nrow(d), replace = TRUE), ]
    coef(lm(model_formula, data = sampled_data))
  })
  
  # processing bootstrapped results
  boot_coefs <- t(boot_coefs)
  boot_mean <- colMeans(boot_coefs)
  boot_error <- apply(boot_coefs, 2, sd)
  
  # bootstrapped CIs
  ci_probs <- c((1 - conf.level)/2, 1 - (1 - conf.level)/2)
  boot_ci <- apply(boot_coefs, 2, quantile, probs = ci_probs)
  
  # table for comparing results (idea from Jonathan!)
  comparison_table <- data.frame(
    Coefficient = names(coef(og_model)),
    Full_Estimate = round(coef(og_model), 3),
    Full_SE = round(og_summary$coefficients[, "Std. Error"], 3),
    Full_CI_Low = round(og_ci[, 1], 3),
    Full_CI_High = round(og_ci[, 2], 3),
    Boot_Mean = round(boot_mean, 3),
    Boot_SE = round(boot_error, 3),
    Boot_CI_Low = round(boot_ci[1, ], 3),
    Boot_CI_High = round(boot_ci[2, ], 3)
  )
  return(comparison_table)
}

# how to use this function in an example? 
bootstrap_function(data, "logHR~logBM")
```

## EXTRA EXTRA CREDIT

*Graph each beta value from the linear model and its corresponding mean value, lower CI and upper CI from a bootstrap as a function of number of bootstraps from 10 to 200 by 10s. HINT: the beta value from the linear model will be the same for all bootstraps and the mean beta value may not differ that much!*
```{r}
library(ggplot2)
```

```{r}
bootstrap_graph <- function(d, m, max_n = 200, step = 10) {
  model_formula <- as.formula(m)
  model <- lm(model_formula, data = d)
  beta_coef <- coef(model)
  results <- data.frame()
  
  # making bootstrap replicates
  for (n in seq(10, max_n, by = step)) {
    coefs <- replicate(n, {
      boot_sample <- d[sample(1:nrow(d), replace = TRUE), ]
      coef(lm(model_formula, data = boot_sample))
    })
    
    # processing bootstrapped results
    coefs <- t(coefs)
    ci <- apply(coefs, 2, quantile, probs = c(0.025, 0.975))
    means <- colMeans(coefs)
    
    # storing results (for each coefficient)
for (i in 1:length(means)) {
      results <- rbind(results, data.frame(
        Coefficient = names(means)[i],
        N = n,
        Mean = means[i],
        CI_Low = ci[1, i],
        CI_High = ci[2, i],
        Full_Beta = beta_coef[i]
      ))
    }
  }
  return(results)
}

# example using my function!
boot_data <- bootstrap_graph(data, "logHR ~ logBM")

# making graph
boot_figure <- ggplot(boot_data, aes(x = N)) +
  geom_line(aes(y = Mean, color = Coefficient)) +
  geom_ribbon(aes(ymin = CI_Low, ymax = CI_High, fill = Coefficient), alpha = 0.5) +
  geom_hline(aes(yintercept = Full_Beta, color = Coefficient), linetype = "dashed") +
  labs(title = "Graphed Beta Values Across Linear Models",
       x = "Number of Bootstraps", 
       y = "Coefficient Value") +
  theme_minimal(base_size = 12)

print(boot_figure)
```

> Carly: After reviewing Jonathan's code, this task seems much more manageable. However, this is still a little above my head and I tried to give it my best attempt.

## Challenges That I Faced: 
- Bootstrapping in general. I still don't feel the most comfortable with it but like anything regarding coding that I've learned this semester, it will likely take more practice!
- I pretty much relied on the textbook R in Action for a defined summary/walk through of how to perform this function, which I found to be helpful when before I was very confused.
- Overall, I felt good about the actual creation of the model! Interpreting data can sometimes be a struggle for me because I find statistics to be tricky but this seemed pretty straightforward! 
- Again, trying to interpret the SE of the mean and why the calculations are different for the estimate vs the actual 1000 samples was an adjustment, just because on the different components of the data that I was working with.
- I did not initially attempt the extra and extra extra credit (creating functions is scary), in my final version I have attempted these two with the help of my peer commentary reviewer who had very clear examples of a great way to perform these tasks. I looked to Jonathan's work as inspiration for this portion of code. 
